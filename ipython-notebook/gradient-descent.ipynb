{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent (спускане по градиента)\n",
    "\n",
    "$$\\displaystyle \\lim_{\\Delta x \\to 0} \\frac{f(x + \\Delta x) - f(x)}{\\Delta x} = \\lim_{\\Delta x \\to 0} \\frac{\\Delta y}{\\Delta x} = f'(x)$$\n",
    "\n",
    "<div class=\"derivative\">\n",
    "    <img src=\"images/derivative.gif\" alt=\"Derivative\" style=\"width: 600px; margin: 10px 0;\"/>\n",
    "</div>\n",
    "\n",
    "- Линейна апроксимация на функция чрез формулата на Тейлър\n",
    "\n",
    "$$f(x+p) = f(x) + p \\, f'(x + \\theta p) ,\\hspace{1pc} \\theta \\in (0, 1) ,\\hspace{1pc} x,p \\in \\mathbb{R}^N$$\n",
    "- Квадратична апроксимация на функция чрез формулата на Тейлър\n",
    "\n",
    "$$f(x+p) = f(x) + p \\, f'(x) + \\frac{1}{2}p^T f''(x + \\theta p)p ,\\hspace{1pc} \\theta \\in (0, 1) ,\\hspace{1pc} x,p \\in \\mathbb{R}^N$$\n",
    "\n",
    "    - Нютонови методи\n",
    "    - Trust Region (области на доверие)\n",
    "    - изискват f(x) да е два пъти диференцируема\n",
    "\n",
    "- Gradient Descent = итеративен метод, който изгражда редица от точки $\\{x_k\\} \\to x^*$ по правилото: \n",
    "\n",
    "$$x_{k+1} = x_k + \\alpha_k p_k$$\n",
    "<center>$p_k = -f'(x_k)^T$ e посоката (на най-бързо спускане, hence gradient descent) </center>\n",
    "<center>$\\alpha_k > 0 \\in \\mathbb{R}$ е стъпкатa </center>\n",
    "\n",
    "- Line Search (линейно търсене на оптимална стъпка)\n",
    "    - Условия на Волф\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\alpha_k > 0 \\, \\in \\, \\mathbb{R}}{\\text{minimize}}\n",
    "& f(x_k + \\alpha_k p_k) \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& f(x_{k+1}) \\leq f(x_k) + c_1 \\alpha_k f'(x_k) p_k & Armijo \\\\\n",
    "& f'(x_{k+1}) p_k \\geq c_2 f'(x_k) p_k & curvature \\\\\n",
    "& 0 < c_1 < c_2 < 1\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "<div class=\"wolfe\">\n",
    "    <img src=\"images/wolfe.png\" alt=\"Wolfe Conditions\" style=\"width: 500px; margin: 10px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using the gradient\n",
      "minimum v [2.222617517893592e-06, -2.963490023858121e-06, 3.33392627684039e-06]\n",
      "minimum value 2.4837366171760904e-11\n",
      "\n",
      "using minimize_batch\n",
      "minimum v [-0.0007788445287802246, -0.0007788445287802246, -0.001038459371706966]\n",
      "minimum value 2.2915954667078066e-06\n",
      "\n",
      "Rosenbrock minimum batch point [0.9799559824593174, 0.9601685501281534]\n",
      "Rosenbrock minimum batch value 0.00040387028777924636\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from functools import reduce\n",
    "import math, random\n",
    "\n",
    "def dot(v, w):\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "def squared_distance(v, w):\n",
    "    return sum_of_squares(vector_subtract(v, w))\n",
    "\n",
    "def distance(v, w):\n",
    "    return math.sqrt(squared_distance(v, w))\n",
    "\n",
    "def vector_subtract(v, w):\n",
    "    \"\"\"subtracts two vectors componentwise\"\"\"\n",
    "    return [v_i - w_i for v_i, w_i in zip(v,w)]\n",
    "def scalar_multiply(c, v):\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"computes the sum of squared elements in v\"\"\"\n",
    "    return sum(v_i ** 2 for v_i in v)\n",
    "\n",
    "def difference_quotient(f, x, h):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "def partial_difference_quotient(f, v, i, h):\n",
    "    # add h to just the i-th element of v\n",
    "    w = [v_j + (h if j == i else 0)\n",
    "         for j, v_j in enumerate(v)]\n",
    "\n",
    "    return (f(w) - f(v)) / h\n",
    "\n",
    "def estimate_gradient(f, v, h=0.00001):\n",
    "    return [partial_difference_quotient(f, v, i, h)\n",
    "            for i, _ in enumerate(v)]\n",
    "\n",
    "def step(v, direction, step_size):\n",
    "    \"\"\"move step_size in the direction from v\"\"\"\n",
    "    return [v_i + step_size * direction_i\n",
    "            for v_i, direction_i in zip(v, direction)]\n",
    "\n",
    "def sum_of_squares_gradient(v):\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "def rosenbrock(v, a=1, b=100):\n",
    "    return (a - v[0])**2 + b*(v[1] - v[0]**2)**2\n",
    "\n",
    "def rosenbrock_gradient(v, a=1, b=100):\n",
    "    return [-2*a + 4*b*v[0]**3 - 4*b*v[0]*v[1] + 2*v[0], 2*b*(v[1] - v[0]**2)]\n",
    "\n",
    "def safe(f):\n",
    "    \"\"\"define a new function that wraps f and return it\"\"\"\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf')         # this means \"infinity\" in Python\n",
    "    return safe_f\n",
    "\n",
    "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    \"\"\"use gradient descent to find theta that minimizes target function\"\"\"\n",
    "\n",
    "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "    theta = theta_0                           # set theta to initial value\n",
    "    target_fn = safe(target_fn)               # safe version of target_fn\n",
    "    value = target_fn(theta)                  # value we're minimizing\n",
    "\n",
    "    while True:\n",
    "        gradient = gradient_fn(theta)\n",
    "        next_thetas = [step(theta, gradient, -step_size)\n",
    "                       for step_size in step_sizes]\n",
    "\n",
    "        # choose the one that minimizes the error function\n",
    "        next_theta = min(next_thetas, key=target_fn)\n",
    "        next_value = target_fn(next_theta)\n",
    "\n",
    "        # stop if we're \"converging\"\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta, value = next_theta, next_value\n",
    "\n",
    "def negate(f):\n",
    "    \"\"\"return a function that for any input x returns -f(x)\"\"\"\n",
    "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
    "\n",
    "def negate_all(f):\n",
    "    \"\"\"the same when f returns a list of numbers\"\"\"\n",
    "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]\n",
    "\n",
    "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    return minimize_batch(negate(target_fn),\n",
    "                          negate_all(gradient_fn),\n",
    "                          theta_0,\n",
    "                          tolerance)\n",
    "\n",
    "print(\"using the gradient\")\n",
    "\n",
    "v = [random.randint(-10,10) for i in range(3)]\n",
    "\n",
    "tolerance = 0.0000001\n",
    "\n",
    "while True:\n",
    "    #print v, sum_of_squares(v)\n",
    "    gradient = sum_of_squares_gradient(v)   # compute the gradient at v\n",
    "    next_v = step(v, gradient, -0.01)       # take a negative gradient step\n",
    "    if distance(next_v, v) < tolerance:     # stop if we're converging\n",
    "        break\n",
    "    v = next_v                              # continue if we're not\n",
    "\n",
    "print(\"minimum v\", v)\n",
    "print(\"minimum value\", sum_of_squares(v))\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"using minimize_batch\")\n",
    "\n",
    "v = [random.randint(-10,10) for i in range(3)]\n",
    "\n",
    "v = minimize_batch(sum_of_squares, sum_of_squares_gradient, v)\n",
    "\n",
    "print(\"minimum v\", v)\n",
    "print(\"minimum value\", sum_of_squares(v))\n",
    "\n",
    "v = minimize_batch(rosenbrock, rosenbrock_gradient, [0.1, 1.4])\n",
    "print(\"\\nRosenbrock minimum batch point\", v)\n",
    "print(\"Rosenbrock minimum batch value\", rosenbrock(v))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
